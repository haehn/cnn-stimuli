{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "K.set_image_dim_ordering('th')\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "np.random.seed(6)\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train = y_train / 10.\n",
    "y_test = y_test / 10.\n",
    "num_classes = 1\n",
    "X_train = np.squeeze(X_train, axis=1)\n",
    "X_test = np.squeeze(X_test, axis=1)\n",
    "X_train = np.pad(X_train, ((0, 0), (10, 10), (10, 10)), 'constant')\n",
    "X_test = np.pad(X_test, ((0, 0), (10, 10), (10, 10)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 48, 48, 3)\n",
      "(60000, 3, 48, 48)\n",
      "60000/60000 [==============================] - 57s 944us/step\n",
      "10000/10000 [==============================] - 10s 970us/step\n"
     ]
    }
   ],
   "source": [
    "X_train_3D = np.stack((X_train,)*3, -1)\n",
    "X_test_3D = np.stack((X_test,)*3, -1)\n",
    "print(X_train_3D.shape)\n",
    "X_train_3D = np.transpose(X_train_3D, (0, 3, 1, 2))\n",
    "X_test_3D = np.transpose(X_test_3D, (0, 3, 1, 2))\n",
    "print(X_train_3D.shape)\n",
    "\n",
    "feature_generator = applications.VGG19(include_top=False, weights=None, input_shape=(3,48,48))\n",
    "X_train_3D_features = feature_generator.predict(X_train_3D, verbose=True)\n",
    "X_test_3D_features = feature_generator.predict(X_test_3D, verbose=True)\n",
    "feature_shape = X_train_3D_features.shape[1] * X_train_3D_features.shape[2] * X_train_3D_features.shape[3]\n",
    "X_train = X_train_3D_features.reshape(len(X_train_3D_features), feature_shape)\n",
    "X_test = X_test_3D_features.reshape(len(X_test_3D_features), feature_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = models.Sequential()\n",
    "MLP.add(layers.Flatten(input_shape=feature_generator.output_shape[1:]))\n",
    "MLP.add(layers.Dense(256, activation='relu', input_dim=(3,48,48)))\n",
    "MLP.add(layers.Dropout(0.2))\n",
    "MLP.add(layers.Dense(1, activation='linear')) # REGRESSION <--- HERE WAS 128 but for regression it should be 1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=feature_generator.input, outputs=MLP(feature_generator.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse', 'mae']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto'), \\\n",
    "            keras.callbacks.ModelCheckpoint('/tmp/VGG_MNIST_modelfile.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0932 - mean_squared_error: 0.0932 - mean_absolute_error: 0.2617 - val_loss: 0.0837 - val_mean_squared_error: 0.0837 - val_mean_absolute_error: 0.2533\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08366, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0836 - mean_squared_error: 0.0836 - mean_absolute_error: 0.2525 - val_loss: 0.0836 - val_mean_squared_error: 0.0836 - val_mean_absolute_error: 0.2533\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08366 to 0.08357, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0836 - mean_squared_error: 0.0836 - mean_absolute_error: 0.2524 - val_loss: 0.0835 - val_mean_squared_error: 0.0835 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08357 to 0.08347, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0834 - mean_squared_error: 0.0834 - mean_absolute_error: 0.2523 - val_loss: 0.0834 - val_mean_squared_error: 0.0834 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08347 to 0.08336, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 5/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0833 - mean_squared_error: 0.0833 - mean_absolute_error: 0.2522 - val_loss: 0.0832 - val_mean_squared_error: 0.0832 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08336 to 0.08324, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 6/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0832 - mean_squared_error: 0.0832 - mean_absolute_error: 0.2520 - val_loss: 0.0831 - val_mean_squared_error: 0.0831 - val_mean_absolute_error: 0.2525\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08324 to 0.08309, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 7/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0830 - mean_squared_error: 0.0830 - mean_absolute_error: 0.2518 - val_loss: 0.0829 - val_mean_squared_error: 0.0829 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08309 to 0.08291, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 8/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0828 - mean_squared_error: 0.0828 - mean_absolute_error: 0.2517 - val_loss: 0.0827 - val_mean_squared_error: 0.0827 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08291 to 0.08268, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 9/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0825 - mean_squared_error: 0.0825 - mean_absolute_error: 0.2513 - val_loss: 0.0824 - val_mean_squared_error: 0.0824 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08268 to 0.08240, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 10/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0823 - mean_squared_error: 0.0823 - mean_absolute_error: 0.2510 - val_loss: 0.0821 - val_mean_squared_error: 0.0821 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08240 to 0.08205, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 11/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0818 - mean_squared_error: 0.0818 - mean_absolute_error: 0.2505 - val_loss: 0.0815 - val_mean_squared_error: 0.0815 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08205 to 0.08152, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 12/1000\n",
      "60000/60000 [==============================] - 214s 4ms/step - loss: 0.0812 - mean_squared_error: 0.0812 - mean_absolute_error: 0.2497 - val_loss: 0.0807 - val_mean_squared_error: 0.0807 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08152 to 0.08071, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 13/1000\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0803 - mean_squared_error: 0.0803 - mean_absolute_error: 0.2485 - val_loss: 0.0794 - val_mean_squared_error: 0.0794 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08071 to 0.07942, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 14/1000\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0785 - mean_squared_error: 0.0785 - mean_absolute_error: 0.2462 - val_loss: 0.0772 - val_mean_squared_error: 0.0772 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07942 to 0.07719, saving model to /tmp/VGG_MNIST_modelfile.h5\n",
      "Epoch 15/1000\n",
      "41504/60000 [===================>..........] - ETA: 1:02 - loss: 0.0763 - mean_squared_error: 0.0763 - mean_absolute_error: 0.2431"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_3D, y_train, validation_data=(X_test_3D, y_test), callbacks=callbacks, epochs=1000, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.ndarray.flatten(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = (y_test, y_pred)\n",
    "with open('Stats_Files/mnvggreg.p', 'w') as f:\n",
    "    pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
